# -*- coding: utf-8 -*-
"""datafinityyy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xDyO29lUombYl0QhVQXPA-_3-0pZitIc
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import re
import nltk
#import spacy
import string
data = pd.read_csv("/content/drive/MyDrive/New folder/Copy of 7282_1.csv")
data

data["reviews.text"].count()

df_temp = data.drop(["latitude", "longitude", "city",	"reviews.username",	"country", "postalCode","address",	"categories",	"name","reviews.userCity",	"province", "reviews.doRecommend",	"reviews.id", "reviews.userProvince"],axis=1)
df_temp.head()

df = df_temp.loc[(data['reviews.rating']==5) | (data['reviews.rating']==4) | (data['reviews.rating']==3) | (data['reviews.rating']==2) | (data['reviews.rating']==1)]
df['reviews.rating'].value_counts()

df['Sentiment_Class']  = df_temp['reviews.rating'].apply(lambda x: 'positive' if x>3 else 'negative')
df.head()

df["review_lower"] = df["reviews.text"].str.lower()
df.head()

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
", ".join(stopwords.words('english'))
STOPWORDS = set(stopwords.words('english'))
#STOPWORDS.add('AppleSupport')
def remove_stopwords(review):
    """custom function to remove the stopwords"""
    return " ".join([word for word in str(review).split() if word not in STOPWORDS])
df["review_wo_stop"] = df["review_lower"].apply(lambda text: remove_stopwords(text))
df.head()

import re
def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)

df["review_wo_html"] = df["review_wo_stop"].apply(lambda text: remove_html(text))
df.head()

PUNCT_TO_REMOVE = string.punctuation
def remove_punctuation(review):
    """custom function to remove the punctuation"""
    return review.translate(str.maketrans('', '', PUNCT_TO_REMOVE))

df["review_wo_punct"] = df["review_wo_html"].apply(lambda review: remove_punctuation(review))
df.head()

import nltk
nltk.download('omw-1.4')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
lemmatizer = WordNetLemmatizer()
nltk.download('averaged_perceptron_tagger')
lemmatizer = WordNetLemmatizer()
wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV}
def lemmatize_words(review):
    pos_tagged_review = nltk.pos_tag(review.split())
    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_review])

df["review_lemmatized_new"] = df["review_wo_punct"].apply(lambda review: lemmatize_words(review))
df.head()

df['review_lemmatized_new'][1]

df.shape

"""## Getting Sentiment Score Using TextBlob"""

pip install textblob

from textblob import TextBlob
df['TextBlob_Pol'] = 0.0

def textblobpol(text):
  sent = TextBlob(text)
  return sent.sentiment.polarity

df['TextBlob_Pol']  = df['review_lemmatized_new'].apply(lambda text: textblobpol(text))

df.head()

df['TextBlob_Pol_Class']  = df['TextBlob_Pol'].apply(lambda x: 'positive' if x>=0 else 'negative')
df['TextBlob_Pol_Class'].value_counts()

df.head()

"""## Getting Sentiment Score Using Sentiment Intensity Analyzer"""

import nltk
nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()

a = 'This was a  very good movie'
#sid.polarity_scores(a)['compound']
sid.polarity_scores(a)

df['SIA_CompundScore'] = df['review_lemmatized_new'].apply(lambda text: sid.polarity_scores(text)['compound'])
df.head()

df['SIA_Class']  = df['SIA_CompundScore'].apply(lambda x: 'positive' if x>=0 else 'negative')

df['SIA_Class'].value_counts()

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(df['Sentiment_Class'], df['TextBlob_Pol_Class'], labels = ['positive','negative'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels =['positive','negative'])
disp.plot()

plt.show()

cm[0,0],cm[1,1]

sum(sum(cm))

print("Accuracy:", (cm[0,0]+cm[1,1])/(sum(sum(cm)))*100)

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(df['Sentiment_Class'], df['SIA_Class'], labels = ['positive','negative'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels =['positive','negative'])
disp.plot()

plt.show()

print("Accuracy:", (cm[0,0]+cm[1,1])/(sum(sum(cm)))*100)

"""## Creating Word Cloud"""

from wordcloud import WordCloud

text = df['review_lemmatized_new'].values
plt.figure(figsize = (5,5))
word_cloud = WordCloud(width = 1000, height = 800,collocations = False, background_color = 'white').generate(str(text))
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""## Getting Sentiment Score Using Flair"""

pip install flair

from flair.models import TextClassifier
from flair.data import Sentence

classifier = TextClassifier.load('en-sentiment')
sentence = Sentence('Kill the darkness within')
classifier.predict(sentence)

# print sentence with predicted labels
print('Sentence above is: ', sentence.labels)

sentence = Sentence('Kill the demon inside')
classifier.predict(sentence)

# print sentence with predicted labels
print('Sentence above is: ', sentence.labels)

from textblob import TextBlob
def textblobpol(text):
  sent = TextBlob(text)
  return sent.sentiment.polarity

print(textblobpol('Kill the darkness within'))

sid.polarity_scores('Kill the darkness within')

df.head()

z = df.loc[df['review_lemmatized_new']==""].index
z

df.drop(z, axis = 0, inplace = True)
df

df.loc[df['review_lemmatized_new']==""]

def get_flair_sent(text):
  sentence = Sentence(text)
  classifier.predict(sentence)
  return sentence.tag.lower()

df['Flair_Class']  = df['review_lemmatized_new'].apply(lambda text: get_flair_sent(str(text)))
df.head()

df['Flair_Class'].value_counts()

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(df['Sentiment_Class'], df['Flair_Class'], labels = ['positive','negative'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels =['positive','negative'])
disp.plot()

plt.show()

print("Accuracy:", (cm[0,0]+cm[1,1])/(sum(sum(cm)))*100)

"""Emotion Analytics"""

pip install transformers

from transformers import pipeline
classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", top_k=None, tokenizer="j-hartmann/emotion-english-distilroberta-base", max_length=512, truncation=True)
prediction = classifier("I love using transformers. The best part is wide range of support and its easy to use", )
print(prediction)

import itertools
def bert_emo(text):
  prediction = classifier(text)
  flattenlist = list(itertools.chain(*prediction))
  data = pd.DataFrame.from_dict(flattenlist)
  data.sort_values(by = 'label', axis = 0, inplace = True)
  score = data['score'].values
  return score

df['bert_emo_anger'],df['bert_emo_disgust'],df['bert_emo_fear'], df['bert_emo_joy'],df['bert_emo_neutral'],\
df['bert_emo_sadness'], df['bert_emo_surprise'] =zip(*df['reviews.text'].apply(lambda x: bert_emo(str(x))))
df

df['bert_emotion'] = df[['bert_emo_anger','bert_emo_disgust','bert_emo_fear','bert_emo_joy',\
                         'bert_emo_neutral','bert_emo_sadness','bert_emo_surprise']].idxmax(axis=1)
df

# Commented out IPython magic to ensure Python compatibility.
# %cd https://colab.research.google.com/drive/1xDyO29lUombYl0QhVQXPA-_3-0pZitIc#scrollTo=A9AVaeM5xddC